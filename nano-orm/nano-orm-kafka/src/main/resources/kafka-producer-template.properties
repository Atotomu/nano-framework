# 用于建立与kafka集群连接的host/port组。数据将会在所有servers上均衡加载，不管哪些server是指定用于bootstrapping。
# 这个列表仅仅影响初始化的hosts（用于发现全部的servers）。这个列表格式：host1:port1,host2:port2,...
# 因为这些server仅仅是用于初始化的连接，以发现集群所有成员关系（可能会动态的变化），这个列表不需要包含所有的servers（你可能想要不止一个server，
# 尽管这样，可能某个server宕机了）。如果没有server在这个列表出现，则发送数据会一直失败，直到列表可用。
# Type: list
# Default: 
# Importance: high
bootstrap.servers=

# Key的序列化类。
# Type: class
# Default: 
# Importance: high
key.serializer=

# Value的序列化类。
# Type: class
# Default: 
# Importance: high
value.serializer=

# producer需要server接收到数据之后发出的确认接收的信号，此项配置就是指procuder需要多少个这样的确认信号。此配置实际上代表了数据备份的可用性。
# 以下设置为常用选项：
#（1）acks=0： 设置为0表示producer不需要等待任何确认收到的信息。副本将立即加到socket  buffer并认为已经发送。
#    没有任何保障可以保证此种情况下server已经成功接收数据，同时重试配置不会发生作用（因为客户端不知道是否失败）回馈的offset会总是设置为-1；
#（2）acks=1： 这意味着至少要等待leader已经成功将数据写入本地log，但是并没有等待所有follower是否成功写入。
#    这种情况下，如果follower没有成功备份数据，而此时leader又挂掉，则消息会丢失。
#（3）acks=all： 这意味着leader需要等待所有备份都成功写入日志，这种策略会保证只要有一个备份存活就不会丢失数据。这是最强的保证。
#（4）其他的设置，例如acks=2也是可以的，这将需要给定的acks数量，但是这种策略一般很少用。
# Type: string
# Default: 1
# Importance: high
acks=1

#producer可以用来缓存数据的内存大小。如果数据产生速度大于向broker发送的速度，producer会阻塞或者抛出异常，以“block.on.buffer.full”来表明。
# 这项设置将和producer能够使用的总内存相关，但并不是一个硬性的限制，因为不是producer使用的所有内存都是用于缓存。
# 一些额外的内存会用于压缩（如果引入压缩机制），同样还有一些用于维护请求。
# Type: long
# Default: 33554432
# Importance: high
buffer.memory=33554432

# producer用于压缩数据的压缩类型。默认是无压缩。正确的选项值是none、gzip、snappy。
# 压缩最好用于批量处理，批量处理消息越多，压缩性能越好。
# Type: string
# Default: none
# Importance: high
compression.type=none

# 设置大于0的值将使客户端重新发送任何数据，一旦这些数据发送失败。注意，这些重试与客户端接收到发送错误时的重试没有什么不同。
# 允许重试将潜在的改变数据的顺序，如果这两个消息记录都是发送到同一个partition，则第一个消息失败第二个发送成功，则第二条消息会比第一条消息出现要早。
# Type: int
# Default: 0
# Importance: high
retries=0

# The password of the private key in the key store file. This is optional for client.
# Type: password
# Default: null
# Importance: high
ssl.key.password=

# The location of the key store file. This is optional for client and can be used for two-way authentication for client.
# Type: string
# Default: null
# Importance: high
ssl.keystore.location=

# The store password for the key store file. 
# This is optional for client and only needed if ssl.keystore.location is configured.
# Type: password
# Default: null
# Importance: high
ssl.keystore.password=

# The location of the trust store file.
# Type: string
# Default: null
# Importance: high
ssl.truststore.location=

# The password for the trust store file. 
# If a password is not set access to the truststore is still available, but integrity checking is disabled.
# Type: password
# Default: null
# Importance: high
ssl.truststore.password=

# producer将试图批处理消息记录，以减少请求次数。这将改善client与server之间的性能。这项配置控制默认的批量处理消息字节数。
# 不会试图处理大于这个字节数的消息字节数。
# 发送到brokers的请求将包含多个批量处理，其中会包含对每个partition的一个请求。
# 较小的批量处理数值比较少用，并且可能降低吞吐量（0则会仅用批量处理）。
# 较大的批量处理数值将会浪费更多内存空间，这样就需要分配特定批量处理数值的内存大小。
# Type: int
# Default: 16384
# Importance: medium
batch.size=16384

# 当向server发出请求时，这个字符串会发送给server。目的是能够追踪请求源头，以此来允许ip/port许可列表之外的一些应用可以发送信息。
# 这项应用可以设置任意字符串，因为没有任何功能性的目的，除了记录和跟踪
# Type: string
# Default: 
# Importance: medium
client.id=

# Close idle connections after the number of milliseconds specified by this config.
# Type: long
# Default: 540000
# Importance: medium
connections.max.idle.ms=540000

# producer组将会汇总任何在请求与发送之间到达的消息记录一个单独批量的请求。
# 通常来说，这只有在记录产生速度大于发送速度的时候才能发生。然而，在某些条件下，客户端将希望降低请求的数量，甚至降低到中等负载一下。
# 这项设置将通过增加小的延迟来完成--即，不是立即发送一条记录，producer将会等待给定的延迟时间以允许其他消息记录发送，这些消息记录可以批量处理。
# 这可以认为是TCP种Nagle的算法类似。
# 这项设置设定了批量处理的更高的延迟边界：
# 一旦我们获得某个partition的batch.size，他将会立即发送而不顾这项设置，然而如果我们获得消息字节数比这项设置要小的多，
# 我们需要“linger”特定的时间以获取更多的消息。 
# 这个设置默认为0，即没有延迟。设定linger.ms=5，
# 例如，将会减少请求数目，但是同时会增加5ms的延迟。
# Type: long
# Default: 0
# Importance: medium
linger.ms=0

# The configuration controls how 
#   long KafkaProducer.send() and KafkaProducer.partitionsFor() will block.
# These methods can be blocked either because the buffer is full or metadata unavailable.
# Blocking in the user-supplied serializers or partitioner will not be counted against this timeout.
# Type: long
# Default: 60000
# Importance: medium
max.block.ms=60000

# 请求的最大字节数。这也是对最大记录尺寸的有效覆盖。
# 注意：server具有自己对消息记录尺寸的覆盖，这些尺寸和这个设置不同。
# 此项设置将会限制producer每次批量发送请求的数目，以防发出巨量的请求。
# Type: int
# Default: 1028576
# Importance: medium
max.request.size=1028576

# Partitioner class that implements the Partitioner interface.
# Type: class
# Default: org.apache.kafka.clients.producer.internals.DefaultPartitioner
# Importance: medium
partitioner.class=org.apache.kafka.clients.producer.internals.DefaultPartitioner

# TCP receive缓存大小，当阅读数据时使用
# Type: int
# Default: 32768
# Importance: medium
receive.buffer.bytes=32768

# The configuration controls the maximum amount of time the client will wait for the response of a request. 
# If the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted. 
# This should be larger than replica.lag.time.max.ms (a broker configuration) to reduce the possibility of message duplication due to unnecessary producer retries.
# Type: int
# Default: 30000
# Importance: medium
request.timeout.ms=30000

# JAAS login context parameters for SASL connections in the format used by JAAS configuration files. 
# JAAS configuration file format is described here. The format for the value is: ' (=)*;'
# Type: password
# Default: null
# Importance: medium
sasl.jaas.config=

# The Kerberos principal name that Kafka runs as. 
# This can be defined either in Kafka's JAAS config or in Kafka's config.
# Type: string
# Default: null
# Importance: medium
sasl.kerberos.service.name=

# SASL mechanism used for client connections. This may be any mechanism for which a security provider is available. 
# GSSAPI is the default mechanism.
# Type: string
# Default: GSSAPI
# Importance: medium
sasl.mechanism=GSSAPI

# Protocol used to communicate with brokers. Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.
# Type: string
# Default: PLAINTEXT
# Importance: medium
security.protocol=PLAINTEXT

# TCP send缓存大小，当发送数据时使用
# Type: int
# Default: 131072
# Importance: medium
send.buffer.bytes=131072

# The list of protocols enabled for SSL connections.
# Type: list
# Default: TLSv1.2,TLSv1.1,TLSv1
# Importance: medium
ssl.enabled.protocols=TLSv1.2,TLSv1.1,TLSv1

# The file format of the key store file. This is optional for client.
# Type: string
# Default: JKS
# Importance: medium
ssl.keystore.type=JKS

# The SSL protocol used to generate the SSLContext. 
# Default setting is TLS, which is fine for most cases. 
# Allowed values in recent JVMs are TLS, TLSv1.1 and TLSv1.2. SSL, SSLv2 and SSLv3 may be supported in older JVMs, 
# but their usage is discouraged due to known security vulnerabilities.
# Type: string
# Default: TLS
# Importance: medium
ssl.protocol=TLS

# The name of the security provider used for SSL connections. Default value is the default security provider of the JVM.
# Type: string
# Default: null
# Importance: medium
ssl.provider=

# The file format of the trust store file.
# Type: string
# Default: JKS
# Importance: medium
ssl.truststore.type=JKS

# When set to 'true', the producer will ensure that exactly one copy of each message is written in the stream. 
# If 'false', producer retries due to broker failures, etc., may write duplicates of the retried message in the stream. 
# This is set to 'false' by default. 
# Note that enabling idempotence requires max.in.flight.requests.per.connection to be set to 1 and retries cannot be zero. 
# Additionally acks must be set to 'all'. If these values are left at their defaults, we will override the default to be suitable. 
# If the values are set to something incompatible with the idempotent producer, a ConfigException will be thrown.
# Type: boolean
# Default: false
# Importance: low
enable.idempotence=false

# A list of classes to use as interceptors. 
# Implementing the ProducerInterceptor interface allows you to intercept (and possibly mutate) the records received by the producer before they are published to the Kafka cluster. 
# By default, there are no interceptors.
# Type: list
# Default: null
# Importance: low
interceptor.classes=

# The maximum number of unacknowledged requests the client will send on a single connection before blocking. 
# Note that if this setting is set to be greater than 1 and there are failed sends, 
# there is a risk of message re-ordering due to retries (i.e., if retries are enabled).
# Type: int
# Default: 5
# Importance: low
max.in.flight.requests.per.connection=5

# 以微秒为单位的时间，是在我们强制更新metadata的时间间隔。即使我们没有看到任何partition leadership改变。
# Type: long
# Default: 300000
# Importance: low
metadata.max.age.ms=300000

# 类的列表，用于衡量指标。实现MetricReporter接口，将允许增加一些类，这些类在新的衡量指标产生时就会改变。JmxReporter总会包含用于注册JMX统计
# Type: list
# Default: []
# Importance: low
metric.reporters=[]

# 用于维护metrics的样本数
# Type: int
# Default: 2
# Importance: low
metrics.num.samples=2

# The highest recording level for metrics.
# Type: string
# Default: INFO
# Importance: low
metrics.recording.level=INFO

# metrics系统维护可配置的样本数量，在一个可修正的window  size。
# 这项配置配置了窗口大小，例如。我们可能在30s的期间维护两个样本。当一个窗口推出后，我们会擦除并重写最老的窗口
# Type: long
# Default: 30000
# Importance: low
metrics.sample.window.ms=30000

# The maximum amount of time in milliseconds to wait when reconnectng to a broker that has repeatedly failed to connect. 
# If provided, the backoff per host will increase exponentially for each consecutive connection failure, up to this maximum. 
# After calculating the backoff increase, 20% random jitter is added to avoid connection storms.
# Type: long
# Default: 1000
# Importance: low
reconnect.backoff.max.ms=1000

# 连接失败时，当我们重新连接时的等待时间。这避免了客户端反复重连
# Type: long
# Default: 50
# Importance: low
recoonect.backoff.ms=50

# 在试图重试失败的produce请求之前的等待时间。避免陷入发送-失败的死循环中。
# Type: long
# Default: 100
# Importance: low
retry.backoff.ms=100

# Kerberos kinit command path.
# Type: string
# Default: /usr/bin/kinit
# Importance: low
sasl.kerberos.kinit.cmd=/usr/bin/kinit

# Login thread sleep time between refresh attempts.
# Type: long
# Default: 60000
# Importance: low
sasl.kerberos.min.time.before.relogin=60000

# Percentage of random jitter added to the renewal time.
# Type: double
# Default: 0.05
# Importance: low
sasl.kerberos.ticket.renew.jitter=0.05

# Login thread will sleep until the specified window factor of time from last refresh to ticket's expiry has been reached, 
# at which time it will try to renew the ticket.
# Type: double
# Default: 0.8
# Importance: low
sasl.kerberos.ticket.renew.window.factor=0.8

# A list of cipher suites. This is a named combination of authentication, encryption, MAC and key exchange algorithm 
# used to negotiate the security settings for a network connection using TLS or SSL network protocol. 
# By default all the available cipher suites are supported.
# Type: list
# Default: null
# Importance: low
ssl.cipher.suites=

# The endpoint identification algorithm to validate server hostname using server certificate.
# Type: string
# Default: null
# Importance: low
ssl.endpoint.identification.algorithm=

# The algorithm used by key manager factory for SSL connections. 
# Default value is the key manager factory algorithm configured for the Java Virtual Machine.
# Type: string
# Default: SunX509
# Importance: low
ssl.keymanager.algorithm=SunX509

# The SecureRandom PRNG implementation to use for SSL cryptography operations.
# Type: string
# Default: null
# Importance: low
ssl.secure.random.implementation=

# The algorithm used by trust manager factory for SSL connections. 
# Default value is the trust manager factory algorithm configured for the Java Virtual Machine.
# Type: string
# Default: PKIX
# Importance: low
ssl.trustmanager.algorithm=PKIX

# The maximum amount of time in ms that the transaction coordinator will wait for a transaction status update from the 
# producer before proactively aborting the ongoing transaction.If this value is larger than the max.transaction.timeout.ms setting in the broker, 
# the request will fail with a `InvalidTransactionTimeout` error.
# Type: int
# Default: 60000
# Importance: low
transaction.timeout.ms=60000

# The TransactionalId to use for transactional delivery. 
# This enables reliability semantics which span multiple producer sessions since it allows the client to guarantee 
# that transactions using the same TransactionalId have been completed prior to starting any new transactions. 
# If no TransactionalId is provided, then the producer is limited to idempotent delivery. 
# Note that enable.idempotence must be enabled if a TransactionalId is configured. 
# The default is empty, which means transactions cannot be used.
# Type: string
# Default: null
# Importance: low
transactional.id=
